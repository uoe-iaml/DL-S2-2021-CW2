{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p style='text-align: right;'>Ver. 1.0</p>**\n",
    "\n",
    "# Introductory Applied Machine Learning (IAML) Coursework 2 - Semester 2, 2020-21\n",
    "\n",
    "### Author: Hiroshi Shimodaira and Jinhong Lu\n",
    "\n",
    "## Important Instructions\n",
    "\n",
    "#### It is important that you follow the instructions below carefully for things to work properly.\n",
    "\n",
    "You need to set up and activate your environment as you would do for your labs, see Learn section on Labs.  **You will need to use Noteable to create one of the files you will submit (the PDF)**.  Do **NOT** create the PDF in some other way, we will not be able to mark it.  If you want to develop your answers in your own environment, you should make sure you are using the same packages we are using, by running the cell which does imports below.\n",
    "\n",
    "Read the instructions in this notebook carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the required code/markdown cell.\n",
    "\n",
    "- We will use the IAML Learn page for any announcements, updates, and FAQs on this assignment. Please visit the page frequently to find the latest information.\n",
    "- Data files that you will be using are located in the ./datasets directory that is included in the [git repository](https://github.com/uoe-iaml/DL-S2-2021-CW2) for this coursework.\n",
    "- There is a helper file 'iaml02cw2_helpers.py' in the git repository, which you should upload to your environment.\n",
    "- Some of the topics in this coursework are covered in weeks 7 and 8 of the course. Focus first on questions on topics that you have covered already, and come back to the other questions as the lectures progress.\n",
    "- Keep your answers brief and concise.\n",
    "- Make sure to show all your code/working.\n",
    "- Write readable code. While we do not expect you to follow PEP8 to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. Do use inline comments when doing something non-standard.\n",
    "- When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. \n",
    "- When you use libraries specified in this coursework, you should use the default parameters unless specified explicitly.\n",
    "- The criteria on which you will be judged include the quality of the textual answers and/or any plots asked for.\n",
    "\n",
    "- You will see <html>\\\\pagebreak</html> at the start of each subquestion.  ***Do not remove these, if you do we will not be able to mark your coursework.***\n",
    "\n",
    "#### Good Scholarly Practice\n",
    "Please remember the University requirement regarding all assessed work for credit. Details about this can be found at:\n",
    "http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct\n",
    "Specifically, this assignment should be your own individual work. We will employ tools for detecting misconduct.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "This assignment will account for 30% of your final mark. We ask you to submit answers to all questions.\n",
    "\n",
    "You will submit (1) a PDF of your Notebook via Gradescope, and (2) the Notebook itself via Learn.  Your grade will be based on the PDF, we will only use the Notebook if we need to see details.  **You must use the following procedure to create the materials to submit**.\n",
    "\n",
    "1. Make sure your Notebook and the datasets are in Noteable and will run.  If you developed your answers in Noteable, this is already done.\n",
    "\n",
    "2. Select **Kernel->Restart & Run All** to create a clean copy of your submission, this will run the cells in order from top to bottom.  This may take a while (a few hours) to complete, ensure that all the output and plots have complete before you proceed.\n",
    "\n",
    "3. Select **File->Download as->PDF via LaTeX (.pdf)** and wait for the PDF to be created and downloaded.\n",
    "\n",
    "4. Select **File->Download as->Notebook (.ipynb)**\n",
    "\n",
    "5. You now should have in your download folder the pdf and the notebook.  Rename them sNNNNNNN.pdf and sNNNNNNN.ipynb, where sNNNNNNN is your matriculation number (student number).\n",
    "\n",
    "**Details on submission instructions will be announced and documented on Learn well before the deadline**. \n",
    "\n",
    "The submission deadline for this assignment is **23rd March 2021 at 16:00 UK time (UTC)**.  Don't leave it to the last minute!\n",
    "\n",
    "### Tips on experiments\n",
    "- Some experiments may take a long time to complete (e.g. more than 10 minutes or could be even more than an hour for Question 3.2 depending on conditions). It will be a good idea that you test your code with a small number of samples for debugging. You should use the whole data when you write your report.\n",
    "\n",
    "#### IMPORTS\n",
    "Execute the cell below to import all packages you will be using for this assignment.  If you are not using Noteable, make sure the python and package version numbers reported match the python and package numbers specified in the comment at the end of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy as np\n",
    "np.random.seed(260393)\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, leaves_list\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All packages imported!\")\n",
    "print(\"python=={}\".format(platform.python_version()))\n",
    "# print(\"seaborn=={}\".format(sns.__version__))\n",
    "print(\"scikit-learn=={}\".format(sklearn.__version__))\n",
    "# print(\"pandas=={}\".format(pd.__version__))\n",
    "print(\"numpy=={}\".format(np.__version__))\n",
    "print(\"scypy=={}\".format(scipy.__version__))\n",
    "print(\"matplotlib=={}\".format(mp.__version__))\n",
    "\n",
    "# You should see this output:\n",
    "# python==3.7.6\n",
    "# seaborn==0.11.0\n",
    "# scikit-learn==0.23.2\n",
    "# pandas==1.1.4\n",
    "# numpy==1.19.4\n",
    "# scypy==1.5.3\n",
    "# matplotlib==3.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Data analysis\n",
    "\n",
    "#### 69 marks out of 163 for this coursework\n",
    "\n",
    "### EMNIST Handwritten Character Dataset\n",
    "\n",
    "This question employs the [EMNIST handwritten character data set](https://www.nist.gov/itl/iad/image-group/emnist-dataset). Each character image is represented as 28-by-28 pixels in gray scale (ranging from 0 to 255), stored as a row vector of 784 elements (28 Ã— 28 = 784) in column-major order. A subset of the original EMNIST data set is considered in the coursework, restricting characters to English alphabet of 26 letters in lower case. Label numbers are given in alphabetical order, where label 0 corresponds to 'a' and 25 to 'z'. There are 1800 training samples and 300 test samples for each class. Note that you will find some errors (wrong labels and wrong letters) in the data set, but we use the data set as it is.\n",
    "\n",
    "***Loading data:***\n",
    "Upload the data set file \"data1.mat\" to your environment, make sure that you have \"iaml02cw2_helpers.py\" in your environment, and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set and apply some changes\n",
    "from iaml02cw2_helpers import *\n",
    "Xtrn_org, Ytrn_org, Xtst_org, Ytst_org = load_EMNIST_subset()\n",
    "Xtrn = Xtrn_org / 255.0   # Training data : (46800, 784)\n",
    "Xtst = Xtst_org / 255.0   # Testing data : (7800, 784)\n",
    "Ytrn = Ytrn_org - 1       # Labels for Xtrn : (46800,)\n",
    "Ytst = Ytst_org - 1       # Labels for Xtst : (7800,)\n",
    "Xmean = np.mean(Xtrn, axis=0)\n",
    "Xtrn_nm = Xtrn - Xmean; Xtst_nm = Xtst - Xmean  # Mean-normalised versions of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtrn and Ytrn are training data and corresponding labels, whereas Xtst and Ytst are test data and labels. Xtrn_nm and Xtst_nm are mean-normalised versions of Xtrn and Xtst, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.1 --- [4 marks] ==========\n",
    "\n",
    "Show the minimum, maximum, mean, and variance of the pixel values for the first sample in Xtrn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.2 --- [6 marks] ==========\n",
    "\n",
    "\n",
    "Display the images of the first and last sample in the training data Xtrn in the following two ways.\n",
    "\n",
    "1. [Code] Display the images using Matplolib's imshow(). You should display images in a gray scale, \n",
    "2. [Code] Display the images using print() function, where you display a character '\\*' when the value of pixel is greater than 0 (zero), and display ' ' (space) otherwise, so that an image is displayed using 28 lines, each of which has the length of 28 characters.\n",
    "\n",
    "Note that an image of 28-by-28 pixels is stored in a vector of 784 elements in ***colum-major order*** instead of row-major order. You need be careful about the order when you recover the original 28-by-28 array from a vector so that the image is displayed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.3 --- [10 marks] ==========\n",
    "\n",
    "Using Xtrn and the Euclidean distance measure, for the first four classes, i.e., 'a','b','c', and 'd', find the two closest samples and four furthest samples of that class to the mean of the class.\n",
    "1. [Code] Display the images of the mean vectors and samples you found in a 4-by-7 grid, where the top row corresponds to the images for class 'a', and the bottom to those for class 'd'. The seven columns are the image of the mean vector and the images of the first and second closest and fourth, third, second, and first furthest samples to the mean vector for that class, respectively (from left to right). For each image sample, you should provide the class number and the sample number in the data set. Note that we use 0-based indexing.\n",
    "2. [Text] Discuss possible issues when we use this data set for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.4 --- [12 marks] ==========\n",
    "\n",
    "Applying the Principal Component Analysis (PCA) to Xtrn_nm using sklearn.decomptision.PCA, answer the following questions:\n",
    "\n",
    "1. [Code] Report the variances of the projected data for the first five principal components.\n",
    "2. [Code] Plot a graph of the cumulative explained variance ratio as a function of the number of principal components, k, where 1 $\\le$ K $\\le$ 784.\n",
    "3. [Code] Find the minimum number of principal components required to explain 50%, 60%, 70%, 80%, and 90% of the total variance, respectively.\n",
    "4. [Code] Display the images of the first 10 principal components in a 2-by-5 grid, putting the image of first principal component on the top left corner, followed by the one of second component to the right. \n",
    "5. [Text] Based on the images you obtained above, discuss your findings briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.5 --- [15 marks] ==========\n",
    "\n",
    "We now consider applying dimensionality reduction with PCA to a sample and reconstructing the sample from the dimensionality-reduced sample.\n",
    "Answer the following questions:\n",
    "\n",
    "1. [Text] Describe this process using mathematical formulae.\n",
    "2. [Code] For each class of 'a', 'b', 'c', and 'd', and for each number of principal components K=5,10,20,40,80,160,320, find the first sample in Xtrn_nm, apply the dimensionality reduction, reconstruct that sample, and display the image of that reconstructed sample in a 4-by-7 grid, where each row corresponds to a class and each column corresponds to a value of K. For each reconstructed sample, you should provide the root mean square error. Note that you should add Xmean to each reconstructed sample to display the corresponding image.\n",
    "3. [Text] Explain your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.6 --- [6 marks] ==========\n",
    "\n",
    "Applying k-means clustering to Xtrn with various values of k, answer (in brief) the following questions:\n",
    "\n",
    "1. [Code] Show a graph of the sum of square error (SSE) as a function of k.\n",
    "2. [Text] Discuss your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.7 --- [16 marks] ==========\n",
    "\n",
    "Apply hierarchical clustering with the Ward's linkage to all the samples of letter 'b' in Xtrn. Answer the following questions.\n",
    "\n",
    " 1. [Code] Plot a dendrogram with scipy.cluster.hierarchy.dendrogram(), in which you specify orientation='right'.\n",
    " 2. [Code] For each of the last four clusters to the root node, i.e, the four nodes from the root node on the dendrogram, find the number of samples (i.e. the number of leaf nodes) that belong to the cluster.\n",
    " 3. [Code] For each of the last four clusters described above, display the image of the cluster centre and the image of the sample that is closest to the cluster centre. For each image sample, you should provide the sample number in the data set. Note that cluster centre is the mean vector of samples in that cluster. \n",
    " 4. [Text] Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2  Classification of handwritten characters\n",
    "\n",
    "#### 62 marks out of 163 for this coursework\n",
    "\n",
    "We use the same data set as the one in Question 1. We use Xtrn_nm for training and Xtst_nm for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.1 --- [14 marks] ==========\n",
    "\n",
    "We consider applying multiclass logistic regression classification to the data set.\n",
    "You should use sklearn.linear_model.LogisticRegression() with the default parameters.\n",
    "Make sure that you use Xtrn_nm for training and Xtst_nm for testing.\n",
    "Answer (in brief) the following questions:  \n",
    "\n",
    "1. [Text] Explain how you can extend the original logistic regression for binary classification to multiclass logistic regression.\n",
    "2. [Code] Carry out a classification experiment with multiclass logistic regression, and report the classification accuracy for the training set and test set.\n",
    "3. [Code] Find the top five classes that were misclassified most in the test set. You should provide the class numbers, corresponding alphabet letters, and the numbers of misclassifications. \n",
    "4. [Text] For each class that you identified in 3 above, make a quick investigation and explain possible reasons for misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.2 --- [10 marks] =========\n",
    "\n",
    "We now look into the effect of the number of training samples ($N$) on classification accuracy using multiclass logistic regression.\n",
    "1. [Code] Carry out a classification experiment for $N$ = 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000, 46800, and plot the accuracy for the training set (Xtrn_nm) and test set (Xtst_nm) on the same graph. Note that you should always use the whole Xtst for testing irrespective of $N$.\n",
    "2. [Text] Discuss your findings based on the graphs (results) you obtained, comparing the graphs for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goest here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.3 --- [6 marks] =========\n",
    "\n",
    "We cosinder applying Support Vector Machines (SVMs) to the data set, using sklearn.svm.SVC().\n",
    "For each of the two conditions shown below, carry out a classification experiment, report classification accuracy and confusion matrix (in numbers instead of in graphical representation such as heatmap) for the test set. You may share code between the two conditions. Make sure that you use Xtrn_nm for training and Xtst_nm for testing.\n",
    "\n",
    "1. [Code] Condition A: SVM with a linear kernel and C=1\n",
    "2. [Code] Condition B: SVM with a RBF kernel and C=1. \n",
    "3. [Text] Discuss your findings, comparing the results for the two conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.4 --- [12 marks] ==========\n",
    "\n",
    "We used default parameters for the SVM in Question 2.2. We now want to tune the parameters by using cross-validation. \n",
    "\n",
    "1. [Text] Explain why we employ cross validation to determine the value of C.\n",
    "2. [Code] To reduce the time for experiments, you pick up the first 400 training samples from each class (from 0 ('a') to 25 ('z') to create Xsmall, so that Xsmall contains 400\\*26=1040 samples in total and its first 400 samples correspond to 'a' and the last 400 to 'z'. Accordingly, you create labels, Ysmall.\n",
    "By using a 3-fold cross validation and Xsmall only, estimate the classification accuracy of an SVM classifier with RBF kernel, while you vary the penalty parameter C in the range $10^{-2}$ to $10^4$ (use 13 values spaced equally log space, where the logarithm base is 10). Set the kernel coefficient parameter gamma to 'auto' for this question. You should use sklearn.model_selection.cross_val_score.\n",
    "Display the mean cross-validation classification accuracy for each C, and plot it against C by using a log-scale for the x-axis.\n",
    "3. [Text] Report the highest obtained mean accuracy score and the value of C which yielded it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.5 --- [20 marks] ==========\n",
    "\n",
    "We now want to improve the classification accuracy for the multiclass logistic regression model from the one obtained in Question 2.1. Answer the following questions.\n",
    "1. [Text] Discuss possible approaches, and decide the one you implement. Note that you should not use other classification models, but you should stick to the multiclass regression approach.\n",
    "2. [Code] Implement the approach you have chosen, carry out a classification experiment, and report accuracy for the training set and test set.\n",
    "3. [Text] Make a quick investigation to the result and report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3  Classification of sequential data\n",
    "\n",
    "#### 32 marks out of 163 for this coursework\n",
    "\n",
    "### Human Activity Recognition data set (UniMiB SHAR)\n",
    "\n",
    "The aim of this task is to predict types of activities of daily living (ADL) from acceleration samples acquired with a smartphone. We use a subset of [UniMiB SHAR Activities](http://www.sal.disco.unimib.it/technologies/unimib-shar/).\n",
    "Activities of 30 people (subjects) were recorded at a sampling frequency of 50Hz and parameterised as a fixed-length sequence of (x,y,z)-accelaration vectors. Each sample is represented as a vector of 453 elements, whose original shape was 151-by-3.\n",
    "There are nine activities:'StandingUpFS', 'StandingUpFL', 'Walking', 'Running', 'GoingUpS', 'Jumping', 'GoingDownS', 'LyingDownFS', 'SittingDown'.\n",
    "\n",
    "For training and evaluation, we employ leave-one-subject-out cross-validation, in which, for each of 30 subjects, the data of the remaining 29 subjects is used for training and the data of the left-out subject is used for validation (testing).\n",
    "We will mainly employ ***macro F1 score*** as the evaluation measure. Note that we can not expect good F1 scores for test data in this question. Scores of around 0.4 are even possible.\n",
    "\n",
    "### Loading data: ###\n",
    "\n",
    "Upload the data set files, 'adl_data.mat', 'adl_labels.mat', 'adl_train_idxssubjective_folds.mat', and 'adl_test_idxssubjective_folds.mat' to your environment, and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data set and apply some changes\n",
    "from iaml02cw2_helpers import *\n",
    "X, Y, train_idx, test_idx = load_UniMiB_SHAR_ADL('.')\n",
    "# Change labels and indices for Python (zero-indexing)\n",
    "Y = Y-1\n",
    "train_idx = train_idx - 1\n",
    "test_idx = test_idx - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- X: whole data (shape=(7579,453))\n",
    "- Y: labels for X (shape=(7579,3)).The first columns is the type of activity, second column is the subject number, and third column is the gender of the subject.\n",
    "- train_idx: list of training data indices for the leave-one-subject-out. For subject k, train_idx\\[k\\] gives indices to X that should be used for training\n",
    "- test_idx:  list of test data indices for the leave-one-subject-out. For subject k, test_idx\\[k\\] gives indices to X that should be used for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 3.1--- [7 marks] ==========\n",
    "\n",
    "Answer the following questions: \n",
    "1. [Code] Using a barplot, plot the frequency of each activity in the whole data set.\n",
    "2. [Text] Explain your findings and discuss possible issues with the data set when we use it for activity classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 3.2--- [10 marks] ==========\n",
    "\n",
    "Answer the following questions: \n",
    "1. [Code] Carry out an experiment with k-nearest neighbours classification with the Euclidean distance measure for k = 1, 2, 3, 5, 7, 10, display *macro F1 scores* for training and testing, and plot the scores on a single figure. Make sure that you employ leave-one-subject-out cross validation.\n",
    "2. [Text] Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 3.3--- [15 marks] ==========\n",
    "\n",
    "We now consider using Gaussian Mixture Models (GMMs) in sklearn, where we employ a GMM for each activity class $C_i$ to estimate $p(X|C_i)$. In classification, we find the class $C_j$ such that $P(X,C_j) > P(X,C_i)$ for $i \\neq j$, where $P(X,C_i) = P(X|C_i)P(C_i)$. We assueme equal prior probabilities, i.e., $P(C_i) = \\frac{1}{9}$ for all $i$ in this question.\n",
    "\n",
    "1. [Code] Train GMMs with diagonal covariance matrices on the training data and calculate the per-sample average log likelihood  and macro-F1 score on the training data and test data for each K = 1,2,3,5,10,20,40, where K denotes the number of mixture components.\n",
    "Display the obtained likelihood values and F1 scores in separate tables, where columns correspond to K and rows correspond to training data and test data, providing a right table title.\n",
    "Plot the likelihoods and F1 scores in separate graphs, where x-axis represents K, using different colours and marks for training and test conditions, and providing a right figure title.\n",
    "Make sure that you employ leave-one-subject-out cross validation to obtained the requested information. Note that, in each cross validation, you should first obtain a per-sample average log likelihood for each class using the data of that class instead of the whole data, and obtain unweighted average over all the classes irrespective of class size. \n",
    "2. [Text] Discuss your findings, compareing the results for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell's output will confirm all cells have been run if you select Kernel->Restart & Run All.\n",
    "# Wait until you see the output printed\n",
    "print(\"*****************************\")\n",
    "print(\"*                           *\")\n",
    "print(\"* All cells have been run!! *\")\n",
    "print(\"*                           *\")\n",
    "print(\"*****************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
